{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import mean, min, max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.108:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb5a81f4e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = spark.read.load('/home/peder/dev/big-data/movie-recommendation-system/data/movies_metadata.csv',\n",
    "                           format=\"csv\",\n",
    "                           sep=\",\",\n",
    "                           inferSchema=\"true\",\n",
    "                           header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adult: string (nullable = true)\n",
      " |-- belongs_to_collection: string (nullable = true)\n",
      " |-- budget: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- homepage: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- original_language: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- poster_path: string (nullable = true)\n",
      " |-- production_companies: string (nullable = true)\n",
      " |-- production_countries: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- spoken_languages: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- tagline: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- video: string (nullable = true)\n",
      " |-- vote_average: string (nullable = true)\n",
      " |-- vote_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is 45572 rows by 24 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df_movies.count(), len(df_movies.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "389\n",
      "985\n"
     ]
    }
   ],
   "source": [
    "print(df_movies.filter(col(\"vote_average\").isNull()).count())\n",
    "print(df_movies.filter(col(\"vote_count\").isNull()).count())\n",
    "print(df_movies.filter(col(\"overview\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = df_movies.na.drop(subset=[\"vote_average\"])\n",
    "df_movies = df_movies.na.drop(subset=[\"vote_count\"])\n",
    "df_movies = df_movies.na.drop(subset=[\"overview\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = df_movies.withColumn(\"vote_average\", df_movies[\"vote_average\"].cast(\"double\"))\n",
    "df_movies = df_movies.withColumn(\"vote_count\", df_movies[\"vote_count\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = df_movies.filter((df_movies.vote_average >=0) & (df_movies.vote_average<=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      vote_average|\n",
      "+-------+------------------+\n",
      "|  count|             40786|\n",
      "|   mean| 5.612511975530867|\n",
      "| stddev|1.9231620784205472|\n",
      "|    min|               0.0|\n",
      "|    max|              10.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.select(['vote_average']).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|       vote_count|\n",
      "+-------+-----------------+\n",
      "|  count|            40760|\n",
      "|   mean|112.1123405299313|\n",
      "| stddev|490.1629388596058|\n",
      "|    min|                0|\n",
      "|    max|            12269|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.select(['vote_count']).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|overview                                                                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_movies.select(['overview']).show(truncate=False, n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(df, column_name=\"content\"):\n",
    "    \"\"\"\n",
    "    This fucntion takes the raw text data and apply a standard NLP preprocessing pipeline consisting of the following steps:\n",
    "      - Text cleaning\n",
    "      - Tokenization\n",
    "      - Stopwords removal\n",
    "      - Stemming (Snowball stemmer)\n",
    "\n",
    "    parameter: dataframe\n",
    "    returns: the input dataframe along with the `cleaned_content` column as the results of the NLP preprocessing pipeline\n",
    "\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, concat_ws\n",
    "    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    # Text preprocessing pipeline\n",
    "    print(\"***** Text Preprocessing Pipeline *****\\n\")\n",
    "\n",
    "    # 1. Text cleaning\n",
    "    print(\"# 1. Text Cleaning\\n\")\n",
    "    # 1.a Case normalization\n",
    "    print(\"1.a Case normalization:\")\n",
    "    lower_case_news_df = df.select(\"id\", lower(col(column_name)).alias(column_name))\n",
    "    lower_case_news_df.show(10)\n",
    "    # 1.b Trimming\n",
    "    print(\"1.b Trimming:\")\n",
    "    trimmed_news_df = lower_case_news_df.select(\"id\", trim(col(column_name)).alias(column_name))\n",
    "    trimmed_news_df.show(10)\n",
    "    # 1.c Filter out punctuation symbols\n",
    "    print(\"1.c Filter out punctuation:\")\n",
    "    no_punct_news_df = trimmed_news_df.select(\"id\", (regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n",
    "    no_punct_news_df.show(10)\n",
    "    # 1.d Filter out any internal extra whitespace\n",
    "    print(\"1.d Filter out extra whitespaces:\")\n",
    "    cleaned_news_df = no_punct_news_df.select(\"id\", trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n",
    "\n",
    "    # 2. Tokenization (split text into tokens)\n",
    "    print(\"# 2. Tokenization:\")\n",
    "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
    "    tokens_df = tokenizer.transform(cleaned_news_df)\n",
    "\n",
    "    print(\"# 3. Stopwords removal:\")\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n",
    "    terms_df = stopwords_remover.transform(tokens_df)\n",
    "\n",
    "    print(\"# 4. Stemming:\")\n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))\n",
    "    \n",
    "    print(\"# 5. untokenize\")\n",
    "    terms_joined_df = terms_stemmed_df.withColumn(\"terms_join\", concat_ws(\" \", \"terms_stemmed\"))\n",
    "    return terms_joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, trim, regexp_replace, concat_ws\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def clean_text_exp(df):    \n",
    "    lower_df = df.withColumn(\"overview_lower\", lower(col(\"overview\")))\n",
    "    trim_df = lower_df.withColumn(\"overview_trim\", trim(col(\"overview_lower\")))\n",
    "    no_punct_df = trim_df.withColumn(\"overview_no_punct\", regexp_replace(col(\"overview_trim\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "    no_whitespace_df = no_punct_df.withColumn(\"overview_no_whitespace\", trim(regexp_replace(col(\"overview_no_punct\"), \" +\", \" \")))\n",
    "    return no_whitespace_df                  \n",
    "    \n",
    "def tokenize_text(df, column_name):\n",
    "    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n",
    "    return tokenizer.transform(df)\n",
    "\n",
    "def stem_tokens(df, column_name):\n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Text Preprocessing Pipeline *****\n",
      "\n",
      "# 1. Text Cleaning\n",
      "\n",
      "1.a Case normalization:\n",
      "+-----+--------------------+\n",
      "|   id|            overview|\n",
      "+-----+--------------------+\n",
      "|  862|led by woody, and...|\n",
      "| 8844|when siblings jud...|\n",
      "|15602|a family wedding ...|\n",
      "|11862|just when george ...|\n",
      "|  949|obsessive master ...|\n",
      "|11860|an ugly duckling ...|\n",
      "|45325|a mischievous you...|\n",
      "| 9091|international act...|\n",
      "|  710|james bond must u...|\n",
      "| 9087|widowed u.s. pres...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1.b Trimming:\n",
      "+-----+--------------------+\n",
      "|   id|            overview|\n",
      "+-----+--------------------+\n",
      "|  862|led by woody, and...|\n",
      "| 8844|when siblings jud...|\n",
      "|15602|a family wedding ...|\n",
      "|11862|just when george ...|\n",
      "|  949|obsessive master ...|\n",
      "|11860|an ugly duckling ...|\n",
      "|45325|a mischievous you...|\n",
      "| 9091|international act...|\n",
      "|  710|james bond must u...|\n",
      "| 9087|widowed u.s. pres...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1.c Filter out punctuation:\n",
      "+-----+--------------------+\n",
      "|   id|            overview|\n",
      "+-----+--------------------+\n",
      "|  862|led by woody andy...|\n",
      "| 8844|when siblings jud...|\n",
      "|15602|a family wedding ...|\n",
      "|11862|just when george ...|\n",
      "|  949|obsessive master ...|\n",
      "|11860|an ugly duckling ...|\n",
      "|45325|a mischievous you...|\n",
      "| 9091|international act...|\n",
      "|  710|james bond must u...|\n",
      "| 9087|widowed us presid...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1.d Filter out extra whitespaces:\n",
      "# 2. Tokenization:\n",
      "# 3. Stopwords removal:\n",
      "# 4. Stemming:\n",
      "# 5. untokenize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, overview: string, tokens: array<string>, terms: array<string>, terms_stemmed: array<string>, terms_join: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = clean_text(df_movies, 'overview')\n",
    "df_clean\n",
    "#from pyspark.ml.feature import Tokenizer\n",
    "#tokenizer = Tokenizer(inputCol=\"overview\", outputCol=\"terms_stemmed\")\n",
    "#tokens_df = tokenizer.transform(df_movies)\n",
    "#df_clean= tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|overview_no_whitespace|\n",
      "+----------------------+\n",
      "|  led by woody andy...|\n",
      "|  when siblings jud...|\n",
      "|  a family wedding ...|\n",
      "|  just when george ...|\n",
      "|  obsessive master ...|\n",
      "|  an ugly duckling ...|\n",
      "|  a mischievous you...|\n",
      "|  international act...|\n",
      "|  james bond must u...|\n",
      "|  widowed us presid...|\n",
      "|  when a lawyer sho...|\n",
      "|  an outcast halfwo...|\n",
      "|  an allstar cast p...|\n",
      "|  morgan adams and ...|\n",
      "|  the life of the g...|\n",
      "|  rich mr dashwood ...|\n",
      "|  its ted the bellh...|\n",
      "|  summoned from an ...|\n",
      "|  a vengeful new yo...|\n",
      "|  an agoraphobic ps...|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean_test = clean_text_exp(df_movies)\n",
    "df_clean_test.select(\"overview_no_whitespace\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      overview_lower|\n",
      "+--------------------+\n",
      "|led by woody andy...|\n",
      "|when siblings jud...|\n",
      "|a family wedding ...|\n",
      "|just when george ...|\n",
      "|obsessive master ...|\n",
      "|an ugly duckling ...|\n",
      "|a mischievous you...|\n",
      "|international act...|\n",
      "|james bond must u...|\n",
      "|widowed us presid...|\n",
      "|when a lawyer sho...|\n",
      "|an outcast halfwo...|\n",
      "|an allstar cast p...|\n",
      "|morgan adams and ...|\n",
      "|the life of the g...|\n",
      "|rich mr dashwood ...|\n",
      "|its ted the bellh...|\n",
      "|summoned from an ...|\n",
      "|a vengeful new yo...|\n",
      "|an agoraphobic ps...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"overview_lower\", lower(col(\"overview\")))\n",
    "df_clean.select(\"overview_lower\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`overview_no_whitespace`' given input columns: [overview, terms, id, terms_join, tokens, terms_stemmed];;\\n'Project ['overview_no_whitespace]\\n+- Project [id#15, overview#1825, tokens#1828, terms#1832, terms_stemmed#1838, concat_ws( , terms_stemmed#1838) AS terms_join#1844]\\n   +- Project [id#15, overview#1825, tokens#1828, terms#1832, <lambda>(terms#1832) AS terms_stemmed#1838]\\n      +- Project [id#15, overview#1825, tokens#1828, UDF(tokens#1828) AS terms#1832]\\n         +- Project [id#15, overview#1825, UDF(overview#1825) AS tokens#1828]\\n            +- Project [id#15, trim(regexp_replace(overview#1811,  +,  ), None) AS overview#1825]\\n               +- Project [id#15, regexp_replace(overview#1797, [^a-zA-Z\\\\s], ) AS overview#1811]\\n                  +- Project [id#15, trim(overview#1783, None) AS overview#1797]\\n                     +- Project [id#15, lower(overview#19) AS overview#1783]\\n                        +- Filter ((vote_average#249 >= cast(0 as double)) && (vote_average#249 <= cast(10 as double)))\\n                           +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, vote_average#249, cast(vote_count#33 as int) AS vote_count#274]\\n                              +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, cast(vote_average#32 as double) AS vote_average#249, vote_count#33]\\n                                 +- Filter AtLeastNNulls(n, overview#19)\\n                                    +- Filter AtLeastNNulls(n, vote_count#33)\\n                                       +- Filter AtLeastNNulls(n, vote_average#32)\\n                                          +- Relation[adult#10,belongs_to_collection#11,budget#12,genres#13,homepage#14,id#15,imdb_id#16,original_language#17,original_title#18,overview#19,popularity#20,poster_path#21,production_companies#22,production_countries#23,release_date#24,revenue#25,runtime#26,spoken_languages#27,status#28,tagline#29,title#30,video#31,vote_average#32,vote_count#33] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o782.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`overview_no_whitespace`' given input columns: [overview, terms, id, terms_join, tokens, terms_stemmed];;\n'Project ['overview_no_whitespace]\n+- Project [id#15, overview#1825, tokens#1828, terms#1832, terms_stemmed#1838, concat_ws( , terms_stemmed#1838) AS terms_join#1844]\n   +- Project [id#15, overview#1825, tokens#1828, terms#1832, <lambda>(terms#1832) AS terms_stemmed#1838]\n      +- Project [id#15, overview#1825, tokens#1828, UDF(tokens#1828) AS terms#1832]\n         +- Project [id#15, overview#1825, UDF(overview#1825) AS tokens#1828]\n            +- Project [id#15, trim(regexp_replace(overview#1811,  +,  ), None) AS overview#1825]\n               +- Project [id#15, regexp_replace(overview#1797, [^a-zA-Z\\s], ) AS overview#1811]\n                  +- Project [id#15, trim(overview#1783, None) AS overview#1797]\n                     +- Project [id#15, lower(overview#19) AS overview#1783]\n                        +- Filter ((vote_average#249 >= cast(0 as double)) && (vote_average#249 <= cast(10 as double)))\n                           +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, vote_average#249, cast(vote_count#33 as int) AS vote_count#274]\n                              +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, cast(vote_average#32 as double) AS vote_average#249, vote_count#33]\n                                 +- Filter AtLeastNNulls(n, overview#19)\n                                    +- Filter AtLeastNNulls(n, vote_count#33)\n                                       +- Filter AtLeastNNulls(n, vote_average#32)\n                                          +- Relation[adult#10,belongs_to_collection#11,budget#12,genres#13,homepage#14,id#15,imdb_id#16,original_language#17,original_title#18,overview#19,popularity#20,poster_path#21,production_companies#22,production_countries#23,release_date#24,revenue#25,runtime#26,spoken_languages#27,status#28,tagline#29,title#30,video#31,vote_average#32,vote_count#33] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2f0baec5acfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overview_no_whitespace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`overview_no_whitespace`' given input columns: [overview, terms, id, terms_join, tokens, terms_stemmed];;\\n'Project ['overview_no_whitespace]\\n+- Project [id#15, overview#1825, tokens#1828, terms#1832, terms_stemmed#1838, concat_ws( , terms_stemmed#1838) AS terms_join#1844]\\n   +- Project [id#15, overview#1825, tokens#1828, terms#1832, <lambda>(terms#1832) AS terms_stemmed#1838]\\n      +- Project [id#15, overview#1825, tokens#1828, UDF(tokens#1828) AS terms#1832]\\n         +- Project [id#15, overview#1825, UDF(overview#1825) AS tokens#1828]\\n            +- Project [id#15, trim(regexp_replace(overview#1811,  +,  ), None) AS overview#1825]\\n               +- Project [id#15, regexp_replace(overview#1797, [^a-zA-Z\\\\s], ) AS overview#1811]\\n                  +- Project [id#15, trim(overview#1783, None) AS overview#1797]\\n                     +- Project [id#15, lower(overview#19) AS overview#1783]\\n                        +- Filter ((vote_average#249 >= cast(0 as double)) && (vote_average#249 <= cast(10 as double)))\\n                           +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, vote_average#249, cast(vote_count#33 as int) AS vote_count#274]\\n                              +- Project [adult#10, belongs_to_collection#11, budget#12, genres#13, homepage#14, id#15, imdb_id#16, original_language#17, original_title#18, overview#19, popularity#20, poster_path#21, production_companies#22, production_countries#23, release_date#24, revenue#25, runtime#26, spoken_languages#27, status#28, tagline#29, title#30, video#31, cast(vote_average#32 as double) AS vote_average#249, vote_count#33]\\n                                 +- Filter AtLeastNNulls(n, overview#19)\\n                                    +- Filter AtLeastNNulls(n, vote_count#33)\\n                                       +- Filter AtLeastNNulls(n, vote_average#32)\\n                                          +- Relation[adult#10,belongs_to_collection#11,budget#12,genres#13,homepage#14,id#15,imdb_id#16,original_language#17,original_title#18,overview#19,popularity#20,poster_path#21,production_companies#22,production_countries#23,release_date#24,revenue#25,runtime#26,spoken_languages#27,status#28,tagline#29,title#30,video#31,vote_average#32,vote_count#33] csv\\n\""
     ]
    }
   ],
   "source": [
    "df_clean.select('overview_no_whitespace').show(truncate=False, n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean.select('terms_join').show(truncate=False, n=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"terms_stemmed\", outputCol=\"tf_features\", vocabSize=20000, minDF=2)\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[cv, idf])\n",
    "features = pipeline.fit(df_clean)\n",
    "tf_idf_features_df = features.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"long\")\n",
    "def num_nonzeros(v):\n",
    "    return v.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total n. of zero-length vectors: {:d}\".\n",
    "      format(tf_idf_features_df.where(num_nonzeros(\"features\") == 0).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features_df = tf_idf_features_df.where(num_nonzeros(\"features\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total n. of zero-length vectors (after removal): {:d}\".\n",
    "      format(tf_idf_features_df.where(num_nonzeros(\"features\") == 0).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features_df.select('features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_vec(a, b):\n",
    "    return 1 - a.dot(b)/(a.norm(2)*b.norm(2))\n",
    "\n",
    "def cosine_similarity(id1, id2):\n",
    "    df_1 = tf_idf_features_df.select(\"features\").where(tf_idf_features_df.id == id1).first()\n",
    "    df_2 = tf_idf_features_df.select(\"features\").where(tf_idf_features_df.id == id2).first()\n",
    "    return cosine_similarity_vec(df_1.features, df_2.features)\n",
    "\n",
    "cosine_similarity(\"238\",\"240\") # toy story 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features_df.select([\"id\",\"title\"]).filter(lower(tf_idf_features_df.title).like('%toy%')).show(truncate=False,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_features_df.select([\"id\",\"title\"]).filter(lower(tf_idf_features_df.title).like('%batman%')).show(truncate=False, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_dataset(movie_vec):\n",
    "    udf_func = udf(lambda df:str(cosine_similarity_vec(movie_vec.features, df)))\n",
    "    return tf_idf_features_df.withColumn(\"cos_sim\", udf_func(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec_batman_forever = tf_idf_features_df.select(\"features\").where(tf_idf_features_df.id == \"414\").first()\n",
    " = cos_sim_dataset(df_vec_batman_forever)\n",
    "df_sim.select([\"cos_sim\", \"id\"]).show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_movies.select([\"title\",\"id\"]).where(df_movies.id == \"8844\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_movies.select([\"id\",\"title\"]).filter(lower(df_movies.title).like('%the godfather%')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_sim.withColumn(\"cos_sim\", df_sim[\"cos_sim\"].cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.orderBy('cos_sim', ascending=False).select([\"cos_sim\", \"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.groupby([\"cos_sim\"]).count().sort(\"cos_sim\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
